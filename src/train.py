import os
import random
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

# âœ… ëª¨ë“ˆ ì„í¬íŠ¸ (ìˆ˜ì •ë¨: encoder -> encoder_test)
from src.dataset import SignLanguageDataset
from src.encoder_test import SLIPVisualEncoder  # íŒŒì¼ëª… ë³€ê²½ ë°˜ì˜
from src.models import HybridTemporalModel, ProtoNetClassifier

# ====================================================
# [1] ë°ì´í„°ì…‹ ì¸ë±ì‹± ìœ í‹¸ë¦¬í‹° (í…ìŠ¤íŠ¸ -> ìˆ«ì ë§¤í•‘)
# ====================================================
def create_class_indices(dataset):
    """
    ë°ì´í„°ì…‹ì„ í•œ ë²ˆ í›‘ì–´ì„œ {ë‹¨ì–´(Text): [ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸]} ë§µì„ ë§Œë“­ë‹ˆë‹¤.
    Few-shot ë°°ì¹˜ë¥¼ ë§Œë“¤ ë•Œ íŠ¹ì • ë‹¨ì–´ì˜ ë°ì´í„°ë¥¼ ë¹¨ë¦¬ ì°¾ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
    """
    print("ğŸ“‚ ë°ì´í„°ì…‹ ì¸ë±ì‹± ì¤‘... (í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ë¶„ë¥˜)")
    class_indices = {}
    
    # dataset ê¸¸ì´ëŠ” __len__ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìŒ
    for idx in tqdm(range(len(dataset))):
        try:
            # __getitem__ì„ í˜¸ì¶œí•˜ë©´ ì˜ìƒ ë¡œë”© ë•Œë¬¸ì— ëŠë¦¬ë¯€ë¡œ
            # dataset ë‚´ë¶€ì˜ json_pathsë¥¼ ì§ì ‘ ì½ì–´ ë¼ë²¨ë§Œ ë¹¼ì˜¤ëŠ” ë°©ì‹ì„ ì”ë‹ˆë‹¤.
            
            json_path = dataset.json_paths[idx]
            # JSON ë¡œë“œ ë¡œì§ ë³µì‚¬ (dataset.py ë¡œì§ ì°¸ì¡°)
            import json
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                label = data['data'][0]['attributes'][0]['name']
                
            if label not in class_indices:
                class_indices[label] = []
            class_indices[label].append(idx)
            
        except Exception as e:
            continue # ì—ëŸ¬ë‚œ ë°ì´í„°ëŠ” ìŠ¤í‚µ

    print(f"âœ… ì¸ë±ì‹± ì™„ë£Œ! ì´ {len(class_indices)}ê°œ í´ë˜ìŠ¤ ë°œê²¬.")
    return class_indices

# ====================================================
# [2] ì—í”¼ì†Œë”• ë°°ì¹˜ ìƒ˜í”ŒëŸ¬ (N-way K-shot)
# ====================================================
def get_episodic_batch(dataset, class_indices, n_way, k_shot, q_query):
    """
    dataset: ì›ë³¸ ë°ì´í„°ì…‹
    class_indices: {ë¼ë²¨: [idx1, idx2...]} ë”•ì…”ë„ˆë¦¬
    """
    # 1. Nê°œì˜ í´ë˜ìŠ¤ ëœë¤ ì„ íƒ
    available_classes = list(class_indices.keys())
    if len(available_classes) < n_way:
        selected_classes = available_classes
        real_n_way = len(selected_classes)
    else:
        selected_classes = random.sample(available_classes, n_way)
        real_n_way = n_way

    support_images = []
    query_images = []
    
    # 2. ê° í´ë˜ìŠ¤ì—ì„œ ë°ì´í„° ë½‘ê¸°
    for i, cls_name in enumerate(selected_classes):
        indices = class_indices[cls_name]
        needed = k_shot + q_query
        
        # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ì¤‘ë³µ í—ˆìš©í•´ì„œ ë½‘ê¸°
        if len(indices) >= needed:
            sampled_idxs = random.sample(indices, needed)
        else:
            sampled_idxs = random.choices(indices, k=needed)
            
        # Support Set (ì •ë‹µì§€)
        for idx in sampled_idxs[:k_shot]:
            # dataset[idx]ëŠ” (frames, label_text)ë¥¼ ë°˜í™˜
            img, _ = dataset[idx] 
            support_images.append(img)
            
        # Query Set (ë¬¸ì œ)
        for idx in sampled_idxs[k_shot:]:
            img, _ = dataset[idx]
            query_images.append(img)
            
    # 3. í…ì„œë¡œ ë³€í™˜
    # datasetì´ [C, T, H, W]ë¥¼ ì£¼ë¯€ë¡œ stackí•˜ë©´ [Batch, C, T, H, W]ê°€ ë¨
    support_images = torch.stack(support_images)
    query_images = torch.stack(query_images)
    
    # ë¼ë²¨ì€ 0ë¶€í„° N-1ê¹Œì§€ ìˆ«ìë¡œ ìƒˆë¡œ ë§Œë“¦ (ì´ë²ˆ ì—í”¼ì†Œë“œìš©)
    support_labels = torch.arange(real_n_way).repeat_interleave(k_shot)
    query_labels = torch.arange(real_n_way).repeat_interleave(q_query)
    
    return support_images, support_labels, query_images, query_labels, real_n_way

# ====================================================
# [3] í•™ìŠµ ì‹¤í–‰
# ====================================================
def train():
    # ì„¤ì • (ê²½ë¡œ ìˆ˜ì •í•˜ì„¸ìš”!)
    DATA_DIR = "/content/drive/MyDrive/Capstone/ìˆ˜ì–´ì˜ìƒ2/18" 
    
    # ë§¥ë¶(MPS) / CUDA / CPU ìë™ ì„ íƒ
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    if torch.backends.mps.is_available(): DEVICE = "mps"
    
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! Device: {DEVICE}")

    # 1. ë°ì´í„°ì…‹ & ì „ì²˜ë¦¬ ì¤€ë¹„
    from torchvision import transforms
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
        # Normalize ë“±ì„ ì¶”ê°€í•˜ë©´ ë” ì¢‹ìŒ
    ])
    
    dataset = SignLanguageDataset(
        data_dir=DATA_DIR, 
        transform=transform, 
        num_frames=16 # SLIP Encoder ì˜ˆì‹œì™€ ë§ì¶¤
    )
    
    # í´ë˜ìŠ¤ë³„ ì¸ë±ìŠ¤ ì •ë¦¬ (Few-shot ë°°ì¹˜ë¥¼ ìœ„í•´ í•„ìˆ˜)
    class_indices = create_class_indices(dataset)
    if len(class_indices) == 0:
        print("âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    # (A) Encoder: SLIPVisualEncoder (íŒŒì¼ëª… encoder_test.py)
    encoder = SLIPVisualEncoder(model_name='vit_base_patch16_224').to(DEVICE)
    
    # (B) Hybrid Model: SLIPì˜ ì¶œë ¥ì°¨ì›(768)ì— ë§ì¶¤
    time_model = HybridTemporalModel(input_dim=encoder.output_dim).to(DEVICE)
    
    # (C) Classifier
    classifier = ProtoNetClassifier().to(DEVICE)
    
    # Optimizer
    optimizer = optim.Adam(
        list(encoder.parameters()) + list(time_model.parameters()), 
        lr=1e-5 # ViTëŠ” í•™ìŠµë¥ ì„ ë‚®ê²Œ ì¡ëŠ” ê²Œ ì¢‹ìŒ
    )

    # 3. í•™ìŠµ ë£¨í”„
    MAX_EPISODES = 100
    N_WAY = 5
    K_SHOT = 1
    Q_QUERY = 1
    
    print("\nğŸ”¥ Training Loop Start...")
    
    for episode in range(MAX_EPISODES):
        try:
            # ë°°ì¹˜ ìƒì„± (ì´ë¯¸ì§€, ë¼ë²¨)
            s_imgs, s_lbls, q_imgs, q_lbls, real_n = get_episodic_batch(
                dataset, class_indices, N_WAY, K_SHOT, Q_QUERY
            )
            
            s_imgs = s_imgs.to(DEVICE) # [N*K, C, T, H, W]
            s_lbls = s_lbls.to(DEVICE)
            q_imgs = q_imgs.to(DEVICE) # [N*Q, C, T, H, W]
            q_lbls = q_lbls.to(DEVICE) # ì •ë‹µ ë¼ë²¨

            # --- Forward Pass ---
            # 1. Encoder (Video -> Frame Features)
            s_feat = encoder(s_imgs) # [N*K, T, 768]
            q_feat = encoder(q_imgs)
            
            # 2. Hybrid Model (Frame Features -> Video Vector)
            s_emb = time_model(s_feat) # [N*K, 768]
            q_emb = time_model(q_feat)
            
            # 3. ProtoNet (ê±°ë¦¬ ê³„ì‚°)
            logits = classifier(s_emb, s_lbls, q_emb, real_n)
            
            # --- Loss & Backward ---
            loss = nn.CrossEntropyLoss()(logits, q_lbls)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if (episode+1) % 10 == 0:
                print(f"Episode [{episode+1}/{MAX_EPISODES}] Loss: {loss.item():.4f}")

        except Exception as e:
            print(f"âš ï¸ Episode {episode} Failed: {e}")
            continue

    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")

if __name__ == "__main__":
    train()