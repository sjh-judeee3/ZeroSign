import os
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from tqdm import tqdm

# âœ… ëª¨ë“ˆ ì„í¬íŠ¸ (íŒŒì¼ëª… ì •í™•íˆ í™•ì¸!)
from dataset import SignLanguageDataset
from encoder import SLIPVideoEncoder 
from models import HybridTemporalModel, ProtoNetClassifier

# ====================================================
# [ì„¤ì •] ê²½ë¡œ ë° íŒŒë¼ë¯¸í„°
# ====================================================
LABEL_DIR = "/content/drive/MyDrive/Capstone/morpheme/01"
VIDEO_DIR = "/content/drive/MyDrive/Capstone/fin_videos_extracted"

MAX_EPISODES = 100  
N_WAY = 5           # 5ì§€ ì„ ë‹¤
K_SHOT = 1          # ì •ë‹µì§€ 1ê°œ
Q_QUERY = 1         # ë¬¸ì œ 1ê°œ
LR = 1e-4           # í•™ìŠµë¥  (Transformerë¼ ì¡°ê¸ˆ ë‚®ì¶¤)

def get_episodic_batch(label_to_indices, dataset, n_way, k_shot, q_query):
    """ì—í”¼ì†Œë“œ(N-way K-shot) ë°°ì¹˜ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    # 1. Nê°œì˜ í´ë˜ìŠ¤ ëœë¤ ì„ íƒ
    valid_labels = [l for l, idxs in label_to_indices.items() if len(idxs) >= k_shot + q_query]
    if len(valid_labels) < n_way: return None, None, None, None
    
    selected_classes = random.sample(valid_labels, n_way)
    
    support_imgs, query_imgs = [], []
    support_labels, query_labels = [], []
    
    for i, class_label in enumerate(selected_classes):
        indices = label_to_indices[class_label]
        selected_indices = random.sample(indices, k_shot + q_query)
        
        # Support Set
        for idx in selected_indices[:k_shot]:
            img, _ = dataset[idx]
            support_imgs.append(img)
            support_labels.append(i)
            
        # Query Set
        for idx in selected_indices[k_shot:]:
            img, _ = dataset[idx]
            query_imgs.append(img)
            query_labels.append(i)
            
    return torch.stack(support_imgs), torch.tensor(support_labels), \
           torch.stack(query_imgs), torch.tensor(query_labels)

def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! Device: {device}")

    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    dataset = SignLanguageDataset(LABEL_DIR, VIDEO_DIR, transform=transform)
    
    # ì¸ë±ì‹± (ì†ë„ ìµœì í™”)
    print("ğŸ“Š ë°ì´í„° ë¶„ë¥˜ ì¤‘...")
    label_to_indices = {}
    for idx in tqdm(range(len(dataset))):
        try:
            import json
            with open(dataset.json_paths[idx], 'r', encoding='utf-8') as f:
                label = json.load(f)['data'][0]['attributes'][0]['name']
            if label not in label_to_indices: label_to_indices[label] = []
            label_to_indices[label].append(idx)
        except: continue

    # 2. ëª¨ë¸ ì´ˆê¸°í™” (3ë‹¨ í•©ì²´!)
    # (A) Encoder: ì´ë¯¸ì§€ -> í”„ë ˆì„ë³„ íŠ¹ì§• (B, T, 512)
    encoder = SLIPVideoEncoder(pretrained=True, embed_dim=512).to(device)
    
    # (B) Temporal: í”„ë ˆì„ë³„ íŠ¹ì§• -> ë¹„ë””ì˜¤ ë²¡í„° (B, 512)
    # models.pyì˜ HybridTemporalModel ì‚¬ìš©
    temporal_model = HybridTemporalModel(input_dim=512, hidden_dim=512).to(device)
    
    # (C) Classifier: ë¹„ë””ì˜¤ ë²¡í„° -> ê±°ë¦¬ ê³„ì‚° & ë¶„ë¥˜
    classifier = ProtoNetClassifier().to(device)
    
    # Optimizer (Encoderì™€ Temporal ëª¨ë¸ ë‘˜ ë‹¤ í•™ìŠµ)
    optimizer = optim.Adam(
        list(encoder.parameters()) + list(temporal_model.parameters()), 
        lr=LR
    )

    # 3. í•™ìŠµ ë£¨í”„
    print("ğŸ”¥ Training Loop Start...")
    model_save_path = "slip_protonet_final.pth"
    
    for episode in range(MAX_EPISODES):
        # ë°°ì¹˜ ìƒì„±
        s_imgs, s_lbls, q_imgs, q_lbls = get_episodic_batch(
            label_to_indices, dataset, N_WAY, K_SHOT, Q_QUERY
        )
        
        if s_imgs is None: 
            print("âŒ í•™ìŠµ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."); break

        s_imgs, s_lbls = s_imgs.to(device), s_lbls.to(device)
        q_imgs, q_lbls = q_imgs.to(device), q_lbls.to(device)

        optimizer.zero_grad()

        # --- Forward Pass (ëª¨ë¸ ì—°ê²°) ---
        # 1. Encoder (Frame Features)
        s_features = encoder(s_imgs) # Output: (N*K, T, 512)
        q_features = encoder(q_imgs) # Output: (N*Q, T, 512)
        
        # 2. Temporal Model (Video Embedding)
        s_emb = temporal_model(s_features) # Output: (N*K, 512)
        q_emb = temporal_model(q_features) # Output: (N*Q, 512)
        
        # 3. ProtoNet Classifier
        # Output: Logits (ìŒìˆ˜ ê±°ë¦¬ê°’)
        logits = classifier(s_emb, s_lbls, q_emb, N_WAY)
        
        # --- Loss & Update ---
        loss = torch.nn.functional.cross_entropy(logits, q_lbls)
        loss.backward()
        optimizer.step()

        # ì •í™•ë„
        acc = (logits.argmax(1) == q_lbls).float().mean()

        if (episode + 1) % 10 == 0:
            print(f"Episode [{episode+1}/{MAX_EPISODES}] Loss: {loss.item():.4f} | Acc: {acc.item()*100:.2f}%")

    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    # ëª¨ë¸ ì €ì¥ (Encoderì™€ Temporal ë‘˜ ë‹¤ ì €ì¥í•´ì•¼ í•¨)
    torch.save({
        'encoder': encoder.state_dict(),
        'temporal': temporal_model.state_dict()
    }, model_save_path)

if __name__ == "__main__":
    train()