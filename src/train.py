## SLIP.py ëë‚˜ë©´ DummyResNetEncoder ëŒ€ì²´í•˜ê¸°

import os
import json
import random
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.models as models_vision

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤
from src.dataset import SignLanguageDataset
from src.models import HybridTemporalModel, ProtoNetClassifier


# [1] ì„ì‹œ ì¸ì½”ë”

class DummyResNetEncoder(nn.Module):
    """
    SLIP ëŒ€ì‹  ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì„œ 512ì°¨ì› íŠ¹ì§•ì„ ë½‘ì•„ì£¼ëŠ” ì„ì‹œ ëª¨ë¸\
    """
    def __init__(self):
        super().__init__()
        # ê°€ë²¼ìš´ ResNet18 ì‚¬ìš©
        resnet = models_vision.resnet18(pretrained=True)
        # ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸°(FC)ë¥¼ ë–¼ì–´ë‚´ê³  íŠ¹ì§•ë§Œ ë½‘ë„ë¡ ìˆ˜ì •
        self.backbone = nn.Sequential(*list(resnet.children())[:-1])
        self.fc = nn.Linear(512, 512) # ì°¨ì› ë§ì¶”ê¸°ìš©

    def forward(self, x):
        # x: [Batch, Frames, 3, 224, 224]
        batch, frames, c, h, w = x.shape
        
        # CNNì€ 4ì°¨ì›ë§Œ ë°›ìœ¼ë¯€ë¡œ [Batch*Frames, 3, 224, 224]ë¡œ í¼ì¹¨
        x_flat = x.view(batch * frames, c, h, w)
        
        # íŠ¹ì§• ì¶”ì¶œ
        feat = self.backbone(x_flat) # [B*F, 512, 1, 1]
        feat = feat.view(batch * frames, -1) # [B*F, 512]
        feat = self.fc(feat) # [B*F, 512]
        
        # ë‹¤ì‹œ ì‹œê°„ ì¶• ë³µêµ¬: [Batch, Frames, 512]
        return feat.view(batch, frames, -1)


# [2] ìœ í‹¸ë¦¬í‹°: ë¼ë²¨ ì‚¬ì „ ë§Œë“¤ê¸° & ë°°ì¹˜ ìƒ˜í”Œë§

def build_label_map(label_dir):
    """
    JSON íŒŒì¼ë“¤ì„ ì½ì–´ì„œ {íŒŒì¼ëª…: ë¼ë²¨ì¸ë±ìŠ¤} ì‚¬ì „ì„ ë§Œë“­ë‹ˆë‹¤.
    """
    print(f" ë¼ë²¨ ë°ì´í„° ì½ëŠ” ì¤‘... ({label_dir})")
    video_to_label = {}
    label_to_idx = {}
    
    # ì˜ˆ: JSON íŒŒì¼ëª…ì´ ë¹„ë””ì˜¤ íŒŒì¼ëª…ê³¼ ëŒ€ì‘ëœë‹¤ê³  ê°€ì •
    # ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ìˆ˜ì • í•„ìš”í•  ìˆ˜ ìˆìŒ
    files = [f for f in os.listdir(label_dir) if f.endswith('.json')]
    
    for json_file in files:
        # 1. JSON ì½ê¸°
        with open(os.path.join(label_dir, json_file), 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # 2. ë‹¨ì–´(Gloss) ì¶”ì¶œ (AI Hub êµ¬ì¡°: attributes -> name)
        # êµ¬ì¡°ê°€ ë³µì¡í•˜ë©´ print(data)ë¡œ í™•ì¸ í•„ìš”
        try:
            # ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ê²½ë¡œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ê°€ì¥ í”í•œ íŒ¨í„´ ì‹œë„:
            word = data.get('attributes', [{}])[0].get('name')
            if not word: continue
            
            # 3. ë¼ë²¨ ì¸ë±ì‹± (í™”ì¥ì‹¤ -> 0, ê°€ë‹¤ -> 1 ...)
            if word not in label_to_idx:
                label_to_idx[word] = len(label_to_idx)
            
            # 4. ë¹„ë””ì˜¤ íŒŒì¼ëª… ë§¤í•‘ (í™•ì¥ìë§Œ json -> mp4ë¡œ ë³€ê²½ ê°€ì •)
            video_name = json_file.replace('.json', '.mp4')
            video_to_label[video_name] = label_to_idx[word]
            
        except Exception as e:
            continue # ì—ëŸ¬ ë‚œ íŒŒì¼ì€ ê±´ë„ˆëœ€

    print(f"âœ… ì´ {len(label_to_idx)}ê°œ ë‹¨ì–´ í´ë˜ìŠ¤ ë°œê²¬!")
    print(f"âœ… ì´ {len(video_to_label)}ê°œ í•™ìŠµìš© ë°ì´í„° ë§¤í•‘ ì™„ë£Œ.")
    return video_to_label, len(label_to_idx)

def get_episodic_batch(dataset, n_way, k_shot, q_query):
    """
    ë°ì´í„°ì…‹ì—ì„œ Nê°œì˜ í´ë˜ìŠ¤ë¥¼ ê³¨ë¼ K+Qê°œì˜ ìƒ˜í”Œì„ ë½‘ì•„ ë°°ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    (ë³µì¡í•œ Sampler ëŒ€ì‹  ê°„ë‹¨í•˜ê²Œ êµ¬í˜„)
    """
    # ë°ì´í„°ì…‹ ì „ì²´ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸° (dataset.label_dict ì—­ì°¸ì¡°ê°€ í•„ìš”í•˜ì§€ë§Œ ê°„ë‹¨íˆ ì²˜ë¦¬)
    # ì‹¤ì œë¡œëŠ” í´ë˜ìŠ¤ë³„ë¡œ ì¸ë±ìŠ¤ë¥¼ ë¯¸ë¦¬ ì •ë¦¬í•´ë‘ëŠ” ê²Œ íš¨ìœ¨ì 
    
    # 1. ì´ë²ˆ ì—í”¼ì†Œë“œì—ì„œ ì‚¬ìš©í•  Nê°œ í´ë˜ìŠ¤ ëœë¤ ì„ íƒ
    available_classes = list(set(dataset.label_dict.values()))
    if len(available_classes) < n_way:
        # í´ë˜ìŠ¤ê°€ ë¶€ì¡±í•˜ë©´ ìˆëŠ” ê±° ë‹¤ ì”€
        selected_classes = available_classes
        real_n_way = len(selected_classes)
    else:
        selected_classes = random.sample(available_classes, n_way)
        real_n_way = n_way

    support_images, support_labels = [], []
    query_images, query_labels = [], []

    # 2. ê° í´ë˜ìŠ¤ë³„ë¡œ ë°ì´í„° ë½‘ê¸°
    class_indices = {} # {label: [idx1, idx2...]}
    for idx, (name, label) in enumerate(dataset.label_dict.items()):
        if label in selected_classes:
            if label not in class_indices: class_indices[label] = []
            class_indices[label].append(idx)

    for i, cls in enumerate(selected_classes):
        indices = class_indices[cls]
        # ë°ì´í„°ê°€ ëª¨ìë¥´ë©´ ì¤‘ë³µ í—ˆìš©í•´ì„œ ë½‘ê¸°
        needed = k_shot + q_query
        if len(indices) >= needed:
            sampled_idxs = random.sample(indices, needed)
        else:
            sampled_idxs = random.choices(indices, k=needed)
            
        # Support Set ë‹´ê¸°
        for idx in sampled_idxs[:k_shot]:
            img, _ = dataset[idx] # img: [Frames, 3, H, W] (dataset.py ìˆ˜ì • í•„ìš”í•  ìˆ˜ ìˆìŒ)
            support_images.append(img)
            support_labels.append(i) # 0 ~ N-1 ë¡œ ì¬ë§¤í•‘
            
        # Query Set ë‹´ê¸°
        for idx in sampled_idxs[k_shot:]:
            img, _ = dataset[idx]
            query_images.append(img)
            query_labels.append(i)

    # í…ì„œë¡œ ë³€í™˜
    support_images = torch.stack(support_images)
    query_images = torch.stack(query_images)
    support_labels = torch.tensor(support_labels)
    
    return support_images, support_labels, query_images, query_labels, real_n_way


# ====================================================
# [3] ë©”ì¸ í•™ìŠµ ì‹¤í–‰ ì½”ë“œ
# ====================================================
def train():
    # ì„¤ì •
    DATA_DIR = "data/raw_videos"
    LABEL_DIR = "data/raw_labels"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    if torch.backends.mps.is_available(): DEVICE = "mps" # ë§¥ë¶ ê°€ì†
    
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! (Device: {DEVICE})")

    # 1. ë°ì´í„° ì¤€ë¹„
    video_label_map, num_classes = build_label_map(LABEL_DIR)
    if len(video_label_map) == 0:
        print("âŒ ë§¤í•‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í´ë” ê²½ë¡œì™€ íŒŒì¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    dataset = SignLanguageDataset(
        video_dir=DATA_DIR,
        label_dict=video_label_map,
        max_frames=30,
        transform=None # í•„ìš” ì‹œ transform ì¶”ê°€
    )
    
    # 2. ëª¨ë¸ ì¤€ë¹„
    # (A) Encoder: ì„ì‹œ ResNet (ë‚˜ì¤‘ì— SLIPìœ¼ë¡œ êµì²´)
    encoder = DummyResNetEncoder().to(DEVICE)
    # (B) Time Model: ìš°ë¦¬ê°€ ë§Œë“  Hybrid ëª¨ë¸
    time_model = HybridTemporalModel(input_dim=512).to(DEVICE)
    # (C) Classifier: ProtoNet
    classifier = ProtoNetClassifier().to(DEVICE)
    
    optimizer = optim.Adam(
        list(encoder.parameters()) + list(time_model.parameters()), 
        lr=1e-4
    )

    # 3. í•™ìŠµ ë£¨í”„ (Episode ë°˜ë³µ)
    N_WAY = 5   # í•œ ë²ˆì— 5ê°œ ë‹¨ì–´ êµ¬ë¶„ ì—°ìŠµ
    K_SHOT = 1  # ì •ë‹µ ì˜ˆì‹œëŠ” 1ê°œë§Œ ë´„
    Q_QUERY = 1 # ë¬¸ì œëŠ” 1ê°œ í’‚
    MAX_EPISODES = 100 # 100ë²ˆ ë°˜ë³µ

    for episode in range(MAX_EPISODES):
        # (1) ë°°ì¹˜ ë§Œë“¤ê¸° (ë³µì¡í•œ ê³¼ì •ì€ í•¨ìˆ˜ê°€ ì²˜ë¦¬)
        try:
            s_img, s_lbl, q_img, _, real_n_way = get_episodic_batch(
                dataset, N_WAY, K_SHOT, Q_QUERY
            )
        except ValueError as e:
            print("âš ï¸ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ (íŒŒì¼ ë¶€ì¡± ë“±):", e)
            continue
            
        s_img = s_img.to(DEVICE) # [N*K, Frames, 3, H, W]
        s_lbl = s_lbl.to(DEVICE)
        q_img = q_img.to(DEVICE) # [N*Q, Frames, 3, H, W]
        
        # (2) Forward Pass
        # ì´ë¯¸ì§€ -> íŠ¹ì§• ë²¡í„° (Encoder)
        s_feat = encoder(s_img) # [N*K, Frames, 512]
        q_feat = encoder(q_img)
        
        # ì‹œí€€ìŠ¤ -> ë¹„ë””ì˜¤ ë²¡í„° (Hybrid Model)
        s_emb = time_model(s_feat) # [N*K, 512]
        q_emb = time_model(q_feat)
        
        # ë¶„ë¥˜ (ProtoNet)
        # ì£¼ì˜: ProtoNetì€ Logits(ìŒìˆ˜ ê±°ë¦¬)ë¥¼ ë°˜í™˜í•¨
        logits = classifier(s_emb, s_lbl, q_emb, real_n_way)
        
        # (3) Loss ê³„ì‚° (ì •ë‹µì€ 0, 1, 2... ìˆœì„œëŒ€ë¡œ ë“¤ì–´ê°)
        # Query ë¼ë²¨ ë§Œë“¤ê¸° (0,0,0... 1,1,1... ì‹)
        target = torch.arange(real_n_way).repeat_interleave(Q_QUERY).to(DEVICE)
        
        loss = nn.CrossEntropyLoss()(logits, target)
        
        # (4) Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (episode+1) % 10 == 0:
            print(f"[{episode+1}/{MAX_EPISODES}] Loss: {loss.item():.4f}")

    print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! (ì„ì‹œ í…ŒìŠ¤íŠ¸ ì„±ê³µ)")

if __name__ == "__main__":
    train()