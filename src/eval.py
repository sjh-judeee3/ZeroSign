import torch
import torch.nn as nn
import numpy as np
import os
import glob
import cv2
from tqdm import tqdm
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# âœ… ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ì„í¬íŠ¸
# (íŒŒì¼ êµ¬ì¡°ê°€ src/encoder.py, src/models.py ë¼ê³  ê°€ì •)
try:
    from src.encoder import SLIPVideoEncoder 
    from src.models import HybridTemporalModel, ProtoNetClassifier
except ImportError:
    # src í´ë” ë‚´ë¶€ì—ì„œ ì‹¤í–‰í•  ê²½ìš°
    from encoder import SLIPVideoEncoder
    from models import HybridTemporalModel, ProtoNetClassifier

# --- [ì„¤ì •] ---
DATA_ROOT = "eval_data_resized"       # ì „ì²˜ë¦¬ëœ ë°ì´í„° í´ë”
CHECKPOINT_PATH = "checkpoints/slip_protonet_final.pth" # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ê²½ë¡œ
N_SUPPORT = 3                         # í´ë˜ìŠ¤ë‹¹ ê¸°ì¤€ ì˜ìƒ ê°œìˆ˜
NUM_FRAMES = 16                       # í•™ìŠµ ë•Œ ì‚¬ìš©í•œ í”„ë ˆì„ ìˆ˜
EMBED_DIM = 512                       # train.pyì˜ embed_dimê³¼ ì¼ì¹˜í•´ì•¼ í•¨
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
if torch.backends.mps.is_available(): DEVICE = "mps" # ë§¥ë¶ìš©

# ==========================================
# 1. í‰ê°€ìš© ë°ì´í„° ë¡œë” (JSON ì—†ì´ MP4 ì§ì ‘ ë¡œë“œ)
# ==========================================
def load_video_tensor(video_path, num_frames=16):
    """
    MP4 íŒŒì¼ì„ ì½ì–´ [1, C, T, H, W] í…ì„œë¡œ ë³€í™˜
    """
    cap = cv2.VideoCapture(video_path)
    frames = []
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames <= 0:
        print(f"Error: Empty video {video_path}")
        return None

    # ê· ë“± ê°„ê²© ìƒ˜í”Œë§ (Uniform Sampling)
    if total_frames <= num_frames:
        indices = np.arange(total_frames)
    else:
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)
    
    current_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret: break
        
        if current_idx in indices:
            # BGR -> RGB & Normalize (0~1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = frame.astype(np.float32) / 255.0
            frames.append(frame)
            if len(frames) == num_frames:
                break
        current_idx += 1
    cap.release()

    # í”„ë ˆì„ ë¶€ì¡± ì‹œ ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì‚¬ (Padding)
    while len(frames) < num_frames:
        frames.append(frames[-1] if frames else np.zeros((224,224,3), dtype=np.float32))

    # Numpy -> Tensor ë³€í™˜
    # frames: [T, H, W, C] -> [C, T, H, W] (Model Input)
    frames = np.array(frames)
    frames = np.transpose(frames, (3, 0, 1, 2)) 
    
    return torch.tensor(frames).unsqueeze(0) # Batch ì°¨ì› ì¶”ê°€ [1, C, T, H, W]

# ==========================================
# 2. ëª¨ë¸ ë¡œë“œ ë° ê°€ì¤‘ì¹˜ ë³µì› (train.py ë°©ì‹ ë°˜ì˜)
# ==========================================
def load_trained_models():
    print(f"ğŸ”„ Loading models on {DEVICE}...")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (train.pyì˜ íŒŒë¼ë¯¸í„°ì™€ ë™ì¼í•˜ê²Œ!)
    encoder = SLIPVideoEncoder(pretrained=False, embed_dim=EMBED_DIM).to(DEVICE)
    time_model = HybridTemporalModel(input_dim=EMBED_DIM, hidden_dim=EMBED_DIM).to(DEVICE)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if not os.path.exists(CHECKPOINT_PATH):
        raise FileNotFoundError(f"Checkpoints not found at {CHECKPOINT_PATH}")
        
    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
    
    # train.pyì—ì„œ ì €ì¥í•œ í‚¤: 'encoder', 'temporal'
    print(f"âœ… Checkpoint Keys Found: {list(checkpoint.keys())}")
    
    try:
        encoder.load_state_dict(checkpoint['encoder'])
        time_model.load_state_dict(checkpoint['temporal'])
        print("âœ… Weights loaded successfully!")
    except KeyError as e:
        print(f"âŒ Key Error loading checkpoint: {e}")
        print("train.pyì˜ ì €ì¥ ì½”ë“œì™€ í‚¤ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None, None
    except Exception as e:
        print(f"âŒ Error loading weights: {e}")
        return None, None

    encoder.eval()
    time_model.eval()
    
    return encoder, time_model

# ==========================================
# 3. í‰ê°€ ì‹¤í–‰
# ==========================================
def run_evaluation():
    # 1. ëª¨ë¸ ì¤€ë¹„
    encoder, time_model = load_trained_models()
    if encoder is None: return
    
    # 2. í´ë˜ìŠ¤ íƒìƒ‰
    classes = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])
    print(f"ğŸ“‚ Found Classes: {classes}")
    
    support_embs = []
    support_lbls = []
    query_embs = []
    query_lbls = [] 

    print("\nğŸš€ Extracting Features & Split Data...")
    
    for label_idx, class_name in enumerate(classes):
        class_dir = os.path.join(DATA_ROOT, class_name)
        video_files = sorted(glob.glob(os.path.join(class_dir, "*.mp4")))
        
        if len(video_files) == 0:
            print(f"âš ï¸  Skipping empty class: {class_name}")
            continue
            
        # ë°ì´í„°ê°€ ë„ˆë¬´ ì ì„ ê²½ìš° ì²˜ë¦¬
        cur_n_support = N_SUPPORT
        if len(video_files) <= N_SUPPORT:
            cur_n_support = 1 # ì˜ìƒì´ ì ìœ¼ë©´ 1ê°œë§Œ Supportë¡œ ì“°ê³  ë‚˜ë¨¸ì§„ Queryë¡œ
            
        s_files = video_files[:cur_n_support]
        q_files = video_files[cur_n_support:]
        
        print(f"   [{class_name}] Support: {len(s_files)} | Query: {len(q_files)}")

        # --- Support Set ì²˜ë¦¬ ---
        for v_path in s_files:
            tensor = load_video_tensor(v_path, NUM_FRAMES)
            if tensor is None: continue
            tensor = tensor.to(DEVICE)
            
            with torch.no_grad():
                f_feat = encoder(tensor) # [1, T, 512]
                vid_emb = time_model(f_feat) # [1, 512]
                
            support_embs.append(vid_emb.cpu())
            support_lbls.append(label_idx)

        # --- Query Set ì²˜ë¦¬ ---
        for v_path in q_files:
            tensor = load_video_tensor(v_path, NUM_FRAMES)
            if tensor is None: continue
            tensor = tensor.to(DEVICE)
            
            with torch.no_grad():
                f_feat = encoder(tensor)
                vid_emb = time_model(f_feat)
                
            query_embs.append(vid_emb.cpu())
            query_lbls.append(label_idx)

    if len(query_lbls) == 0:
        print("âŒ í‰ê°€í•  Query ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë¦¬ìŠ¤íŠ¸ -> í…ì„œ ë³€í™˜
    S = torch.cat(support_embs).to(DEVICE) # [Total_Support, Dim]
    S_Y = torch.tensor(support_lbls).to(DEVICE)
    Q = torch.cat(query_embs).to(DEVICE)   # [Total_Query, Dim]
    Q_Y = np.array(query_lbls)             

    # ==========================================
    # 4. ProtoNet ê±°ë¦¬ ê³„ì‚° ë° ë¶„ë¥˜
    # ==========================================
    classifier = ProtoNetClassifier().to(DEVICE)
    
    # (1) í”„ë¡œí† íƒ€ì… ê³„ì‚°
    num_classes = len(classes)
    prototypes = classifier.compute_prototypes(S, S_Y, num_classes) 
    
    # (2) ê±°ë¦¬ ê³„ì‚°
    dists = classifier.euclidean_distance(Q, prototypes)
    
    # (3) ì˜ˆì¸¡
    predictions = torch.argmin(dists, dim=1).cpu().numpy()
    
    # ==========================================
    # 5. ê²°ê³¼ ì‹œê°í™”
    # ==========================================
    acc = accuracy_score(Q_Y, predictions)
    print(f"\nğŸ† Final Accuracy: {acc * 100:.2f}%")
    print("\n--- Classification Report ---")
    print(classification_report(Q_Y, predictions, target_names=classes))
    
    # Confusion Matrix
    cm = confusion_matrix(Q_Y, predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes)
    plt.title(f'Confusion Matrix (Acc: {acc*100:.1f}%)')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    
    save_path = 'evaluation_result.png'
    plt.savefig(save_path)
    print(f"\nâœ… Result image saved at: {save_path}")
    plt.show()

if __name__ == "__main__":
    run_evaluation()