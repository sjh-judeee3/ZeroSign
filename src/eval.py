import torch
import torch.nn as nn
import numpy as np
import os
import glob
import cv2
from tqdm import tqdm
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# âœ… ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ ì„í¬íŠ¸
# (íŒŒì¼ êµ¬ì¡°ê°€ src/encoder_test.py, src/models.py ë¼ê³  ê°€ì •)
try:
    from src.encoder_test import SLIPVisualEncoder
    from src.models import HybridTemporalModel, ProtoNetClassifier
except ImportError:
    # src í´ë” ë‚´ë¶€ì—ì„œ ì‹¤í–‰í•  ê²½ìš°
    from encoder_test import SLIPVisualEncoder
    from models import HybridTemporalModel, ProtoNetClassifier

# --- [ì„¤ì •] ---
DATA_ROOT = "eval_data_resized"       # ì „ì²˜ë¦¬ëœ ë°ì´í„° í´ë”
CHECKPOINT_PATH = "checkpoints/slip_protonet_final.pth" # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ê²½ë¡œ
N_SUPPORT = 3                         # í´ë˜ìŠ¤ë‹¹ ê¸°ì¤€ ì˜ìƒ ê°œìˆ˜ (Few-shot Support)
NUM_FRAMES = 16                       # ëª¨ë¸ì´ í•™ìŠµí•  ë•Œ ì¼ë˜ í”„ë ˆì„ ìˆ˜
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
if torch.backends.mps.is_available(): DEVICE = "mps" # ë§¥ë¶ìš©

# ==========================================
# 1. í‰ê°€ìš© ë°ì´í„° ë¡œë” (JSON ì—†ì´ MP4 ì§ì ‘ ë¡œë“œ)
# ==========================================
def load_video_tensor(video_path, num_frames=16):
    """
    MP4 íŒŒì¼ì„ ì½ì–´ [1, C, T, H, W] í…ì„œë¡œ ë³€í™˜
    """
    cap = cv2.VideoCapture(video_path)
    frames = []
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames <= 0:
        print(f"Error: Empty video {video_path}")
        return None

    # ê· ë“± ê°„ê²© ìƒ˜í”Œë§ (Uniform Sampling)
    if total_frames <= num_frames:
        indices = np.arange(total_frames)
    else:
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)
    
    current_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret: break
        
        if current_idx in indices:
            # BGR -> RGB & Normalize (0~1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = frame.astype(np.float32) / 255.0
            frames.append(frame)
            if len(frames) == num_frames:
                break
        current_idx += 1
    cap.release()

    # í”„ë ˆì„ ë¶€ì¡± ì‹œ ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì‚¬ (Padding)
    while len(frames) < num_frames:
        frames.append(frames[-1] if frames else np.zeros((224,224,3), dtype=np.float32))

    # Numpy -> Tensor ë³€í™˜
    # frames: [T, H, W, C] -> [C, T, H, W] (Model Input)
    frames = np.array(frames)
    frames = np.transpose(frames, (3, 0, 1, 2)) 
    
    return torch.tensor(frames).unsqueeze(0) # Batch ì°¨ì› ì¶”ê°€ [1, C, T, H, W]

# ==========================================
# 2. ëª¨ë¸ ë¡œë“œ ë° ê°€ì¤‘ì¹˜ ë³µì›
# ==========================================
def load_trained_models():
    print(f"ğŸ”„ Loading models on {DEVICE}...")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    encoder = SLIPVisualEncoder(model_name='vit_base_patch16_224').to(DEVICE)
    time_model = HybridTemporalModel(input_dim=encoder.output_dim).to(DEVICE) # output_dim=768
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if not os.path.exists(CHECKPOINT_PATH):
        raise FileNotFoundError(f"Checkpoints not found at {CHECKPOINT_PATH}")
        
    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
    
    # ì €ì¥ ë°©ì‹ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬
    # Case 1: {'encoder': ..., 'time_model': ...} ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥ëœ ê²½ìš° (ê¶Œì¥)
    if isinstance(checkpoint, dict) and 'encoder_state_dict' in checkpoint:
        print("âœ… Detected dictionary checkpoint format.")
        encoder.load_state_dict(checkpoint['encoder_state_dict'])
        time_model.load_state_dict(checkpoint['time_model_state_dict'])
        
    # Case 2: ëª¨ë¸ ì „ì²´ê°€ ì €ì¥ëœ ê²½ìš° or ë‹¤ë¥¸ í‚¤ê°’
    else:
        print("âš ï¸ Warning: Unknown checkpoint format. Trying direct load...")
        # ë§Œì•½ í‚¤ ê°’ì´ ë‹¤ë¥´ë‹¤ë©´ ì•„ë˜ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤. 
        # ì˜ˆ: checkpoint['model'] ë“±. 
        # ì§€ê¸ˆì€ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìœ¼ë‹ˆ ì¼ë‹¨ íŒ¨ìŠ¤í•˜ê±°ë‚˜ ì‚¬ìš©ì í™•ì¸ í•„ìš”.
        try:
            encoder.load_state_dict(checkpoint['encoder']) # í‚¤ ì´ë¦„ ì¶”ì¸¡
            time_model.load_state_dict(checkpoint['hybrid'])
        except:
            print("âŒ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨. train.pyì—ì„œ ì €ì¥ ë°©ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            print(f"Available keys: {checkpoint.keys() if isinstance(checkpoint, dict) else 'Not a dict'}")

    encoder.eval()
    time_model.eval()
    
    return encoder, time_model

# ==========================================
# 3. í‰ê°€ ì‹¤í–‰ (ProtoNet Logic)
# ==========================================
def run_evaluation():
    # 1. ëª¨ë¸ ì¤€ë¹„
    encoder, time_model = load_trained_models()
    
    # 2. í´ë˜ìŠ¤ íƒìƒ‰
    classes = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])
    print(f"ğŸ“‚ Found Classes: {classes}")
    
    support_embs = []
    support_lbls = []
    query_embs = []
    query_lbls = [] # ì •ë‹µì§€

    print("\nğŸš€ Extracting Features & Split Data (Support vs Query)...")
    
    for label_idx, class_name in enumerate(classes):
        class_dir = os.path.join(DATA_ROOT, class_name)
        video_files = sorted(glob.glob(os.path.join(class_dir, "*.mp4")))
        
        if len(video_files) == 0:
            print(f"âš ï¸  Skipping empty class: {class_name}")
            continue
            
        # ë°ì´í„° ë¶„í•  (ì•ì˜ Nê°œëŠ” Support, ë‚˜ë¨¸ì§€ëŠ” Query)
        # ë§Œì•½ ì˜ìƒì´ 3ê°œ ì´í•˜ë¼ë©´? -> 1ê°œë¥¼ Support, ë‚˜ë¨¸ì§€ë¥¼ Queryë¡œ ê°•ì œ ì¡°ì •
        cur_n_support = N_SUPPORT
        if len(video_files) <= N_SUPPORT:
            cur_n_support = 1
            print(f"âš ï¸  {class_name}: Not enough videos. Using 1 for support.")

        s_files = video_files[:cur_n_support]
        q_files = video_files[cur_n_support:]
        
        print(f"   [{class_name}] Support: {len(s_files)} | Query: {len(q_files)}")

        # --- Support Set ì²˜ë¦¬ ---
        for v_path in s_files:
            tensor = load_video_tensor(v_path, NUM_FRAMES)
            if tensor is None: continue
            tensor = tensor.to(DEVICE)
            
            with torch.no_grad():
                # Encoder (Video -> Frame Features)
                f_feat = encoder(tensor) # [1, T, 768]
                # Time Model (Frame Features -> Video Vector)
                vid_emb = time_model(f_feat) # [1, 768]
                
            support_embs.append(vid_emb.cpu())
            support_lbls.append(label_idx)

        # --- Query Set ì²˜ë¦¬ ---
        for v_path in q_files:
            tensor = load_video_tensor(v_path, NUM_FRAMES)
            if tensor is None: continue
            tensor = tensor.to(DEVICE)
            
            with torch.no_grad():
                f_feat = encoder(tensor)
                vid_emb = time_model(f_feat)
                
            query_embs.append(vid_emb.cpu())
            query_lbls.append(label_idx)

    # ë¦¬ìŠ¤íŠ¸ -> í…ì„œ ë³€í™˜
    S = torch.cat(support_embs).to(DEVICE) # [Total_Support, Dim]
    S_Y = torch.tensor(support_lbls).to(DEVICE)
    Q = torch.cat(query_embs).to(DEVICE)   # [Total_Query, Dim]
    Q_Y = np.array(query_lbls)             # Metric ê³„ì‚°ìš© (numpy)

    # ==========================================
    # 4. ProtoNet ê±°ë¦¬ ê³„ì‚° ë° ë¶„ë¥˜
    # ==========================================
    classifier = ProtoNetClassifier().to(DEVICE)
    
    # (1) í”„ë¡œí† íƒ€ì… ê³„ì‚°
    num_classes = len(classes)
    prototypes = classifier.compute_prototypes(S, S_Y, num_classes) # [N_Class, Dim]
    
    # (2) ê±°ë¦¬ ê³„ì‚° (Query vs Prototypes)
    # Output: [N_Query, N_Classes]
    dists = classifier.euclidean_distance(Q, prototypes)
    
    # (3) ì˜ˆì¸¡ (ê±°ë¦¬ê°€ ê°€ì¥ ì§§ì€ í´ë˜ìŠ¤ ì„ íƒ)
    # distsê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ -> argmin
    predictions = torch.argmin(dists, dim=1).cpu().numpy()
    
    # ==========================================
    # 5. ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥
    # ==========================================
    acc = accuracy_score(Q_Y, predictions)
    print(f"\nğŸ† Final Accuracy: {acc * 100:.2f}%")
    print("\n--- Classification Report ---")
    print(classification_report(Q_Y, predictions, target_names=classes))
    
    # Confusion Matrix
    cm = confusion_matrix(Q_Y, predictions)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes)
    plt.title(f'Confusion Matrix (Acc: {acc*100:.1f}%)')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    
    save_path = 'evaluation_result.png'
    plt.savefig(save_path)
    print(f"\nâœ… Result image saved at: {save_path}")
    plt.show()

if __name__ == "__main__":
    run_evaluation()