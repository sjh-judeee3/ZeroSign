import os
import glob
import cv2
import numpy as np
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# --- [ì„¤ì •] ---
DATA_ROOT = "eval_data_resized"

# MediaPipeëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 7ê°€ì§€ ì œìŠ¤ì²˜ë§Œ ì¸ì‹í•©ë‹ˆë‹¤.
# ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë™ì‘ê³¼ ê°€ì¥ ë¹„ìŠ·í•œ ê²ƒìœ¼ë¡œ ë§¤í•‘í•´ì•¼ ì±„ì ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
# Noneìœ¼ë¡œ ì„¤ì •ëœ ê²ƒì€ MediaPipeê°€ ì•„ì˜ˆ ëª¨ë¥´ëŠ” ë™ì‘ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.
MP_MAPPING = {
    # MediaPipe Output  : Our Class Label
    "Closed_Fist"       : "Grab",
    "Open_Palm"         : "Stop",
    "Pointing_Up"       : "Point",
    "Thumb_Up"          : None, # ìš°ë¦¬ ë°ì´í„°ì—” ì—„ì§€ì²™ ì—†ìŒ -> ì˜¤ë‹µ ì²˜ë¦¬
    "Thumb_Down"        : None,
    "Victory"           : "Pinch", # ê°€ë” Pinchë¥¼ ë¸Œì´(Victory)ë¡œ ì°©ê°í•¨ (ë§¤í•‘í•´ì¤˜ë„ ë¨)
    "ILoveYou"          : None
}

# ìš°ë¦¬ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ì •ë‹µì§€)
OUR_CLASSES = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])

def get_mediapipe_prediction(video_path):
    """
    ì˜ìƒ ì „ì²´ í”„ë ˆì„ì„ ëŒë©´ì„œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ì œìŠ¤ì²˜ë¥¼ ìµœì¢… ì˜ˆì¸¡ìœ¼ë¡œ ì„ ì • (Voting)
    """
    # ëª¨ë¸ ë¡œë“œ (ê°€ì¥ ê°€ë²¼ìš´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
    base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')
    options = vision.GestureRecognizerOptions(base_options=base_options)
    recognizer = vision.GestureRecognizer.create_from_options(options)

    cap = cv2.VideoCapture(video_path)
    predictions = []

    while True:
        ret, frame = cap.read()
        if not ret: break
        
        # MediaPipeìš© ì´ë¯¸ì§€ ë³€í™˜
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)

        # ì¶”ë¡ 
        recognition_result = recognizer.recognize(mp_image)

        if recognition_result.gestures:
            # ê°€ì¥ í™•ì‹ í•˜ëŠ” ì œìŠ¤ì²˜ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            top_gesture = recognition_result.gestures[0][0].category_name
            predictions.append(top_gesture)
        else:
            predictions.append("None")

    cap.release()

    if not predictions:
        return "Unknown"

    # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’(ìµœë¹ˆê°’) ì„ íƒ
    from collections import Counter
    most_common = Counter(predictions).most_common(1)[0][0]
    
    # ìš°ë¦¬ì˜ ë¼ë²¨ë¡œ ë³€í™˜ (Mapping)
    final_pred = MP_MAPPING.get(most_common, "Unknown")
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€(Unknown) ê²°ê³¼ëŠ” ìš°ë¦¬ í´ë˜ìŠ¤ ì¤‘ ì•„ë¬´ê±°ë‚˜ í•˜ë‚˜ë¡œ ì°ê±°ë‚˜(Random),
    # ì˜¤ë‹µ ì²˜ë¦¬ë¥¼ ìœ„í•´ 'Wrong'ì´ë¼ëŠ” ë¼ë²¨ì„ ë‘¡ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì •í™•í•œ ë¹„êµë¥¼ ìœ„í•´ ìš°ë¦¬ í´ë˜ìŠ¤ì— ì—†ìœ¼ë©´ ê·¸ëƒ¥ 'Unknown'ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
    
    return final_pred

def run_baseline_eval():
    # ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
    if not os.path.exists('gesture_recognizer.task'):
        print("ğŸ“¥ Downloading MediaPipe Model...")
        os.system('wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task')

    print(f"ğŸ“‚ Evaluating Baseline (MediaPipe) on {DATA_ROOT}...")
    
    y_true = []
    y_pred = []
    
    # ì¿¼ë¦¬/ì„œí¬íŠ¸ êµ¬ë¶„ ì—†ì´ ì „ì²´ ë°ì´í„°ë¡œ í‰ê°€ (Baselineì€ Few-shotì´ ì•„ë‹ˆë¯€ë¡œ)
    # í•˜ì§€ë§Œ ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ test data ì „ì²´ë¥¼ ë‹¤ ì”ë‹ˆë‹¤.
    
    for class_name in OUR_CLASSES:
        class_dir = os.path.join(DATA_ROOT, class_name)
        video_files = sorted(glob.glob(os.path.join(class_dir, "*.mp4")))
        
        print(f"   Processing {class_name} ({len(video_files)} videos)...")
        
        for v_path in video_files:
            # ì •ë‹µ
            y_true.append(class_name)
            
            # ì˜ˆì¸¡
            pred = get_mediapipe_prediction(v_path)
            
            # ì˜ˆì¸¡ê°’ì´ ìš°ë¦¬ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ (ì˜ˆ: Pinchì¸ë° Unknownì´ë¼ê³  í•¨) -> í‹€ë¦° ê²ƒìœ¼ë¡œ ê°„ì£¼
            # í¸ì˜ìƒ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸ì— ê·¸ëŒ€ë¡œ ë„£ìŠµë‹ˆë‹¤. (ë‚˜ì¤‘ì— Confusion Matrixì—ì„œ ë³„ë„ ì»¬ëŸ¼ìœ¼ë¡œ ëœ¸)
            y_pred.append(pred)

    # --- ê²°ê³¼ ê³„ì‚° ---
    # ì •í™•ë„ ê³„ì‚° ì‹œ Unknownì´ë‚˜ Noneì€ ë¬´ì¡°ê±´ ì˜¤ë‹µ ì²˜ë¦¬ë¨
    acc = accuracy_score(y_true, y_pred)
    print(f"\nğŸ“‰ Baseline Accuracy: {acc * 100:.2f}%")
    
    # Confusion Matrix (Unknown í¬í•¨í•´ì„œ ê·¸ë¦¬ê¸°)
    # y_predì— 'Unknown'ì´ë‚˜ 'Grab' ë“±ì´ ì„ì—¬ ìˆìŒ.
    # ì‹œê°í™”ë¥¼ ìœ„í•´ ë¼ë²¨ ìœ ë‹ˆì˜¨ì„ ë§Œë“¦
    all_labels = sorted(list(set(y_true + y_pred)))
    
    cm = confusion_matrix(y_true, y_pred, labels=all_labels)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', # ì¼ë¶€ëŸ¬ ë¹¨ê°„ìƒ‰ (ê²½ê³  ëŠë‚Œ)
                xticklabels=all_labels, yticklabels=all_labels)
    plt.title(f'Baseline Confusion Matrix (Acc: {acc*100:.1f}%)')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('baseline_result.png')
    plt.show()

if __name__ == "__main__":
    run_baseline_eval()