import os
import json
import glob
import cv2
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image

class SignLanguageDataset(Dataset):
    def __init__(self, label_dir, video_dir, transform=None, num_frames=16):
        """
        Args:
            label_dir (str): JSON ë¼ë²¨ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”
            video_dir (str): MP4 ì˜ìƒ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” (í•˜ìœ„ í´ë” í¬í•¨ ê²€ìƒ‰)
        """
        self.label_dir = label_dir
        self.video_dir = video_dir
        self.transform = transform
        self.num_frames = num_frames
        
        print(f"ğŸ” ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘... (Label: {label_dir}, Video: {video_dir})")

        # 1. ì˜ìƒ íŒŒì¼ ë¯¸ë¦¬ ì°¾ì•„ì„œ ì§€ë„(Map) ë§Œë“¤ê¸°
        # (ì˜ìƒì´ ë” ì ìœ¼ë¯€ë¡œ ì˜ìƒì„ ê¸°ì¤€ìœ¼ë¡œ JSONì„ ì°¾ëŠ” ê²Œ ì•ˆì „í•©ë‹ˆë‹¤)
        self.video_map = {}
        mp4_files = glob.glob(os.path.join(video_dir, "**", "*.mp4"), recursive=True)
        
        for path in mp4_files:
            filename = os.path.basename(path)
            self.video_map[filename] = path
            
        print(f"ğŸ¥ ì˜ìƒ(MP4) íŒŒì¼ {len(self.video_map)}ê°œ ìœ„ì¹˜ í™•ë³´.")

        # 2. ëª¨ë“  JSON íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ì—…
        all_json_paths = glob.glob(os.path.join(label_dir, "**", "*.json"), recursive=True)
        print(f"ğŸ“„ ë°œê²¬ëœ ì „ì²´ ë¼ë²¨(JSON) íŒŒì¼: {len(all_json_paths)}ê°œ")
        
        # 3. [í•µì‹¬ ìˆ˜ì •] ì˜ìƒì´ ì¡´ì¬í•˜ëŠ” JSONë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ (í•„í„°ë§)
        self.json_paths = []
        print("âš™ï¸ ìœ íš¨í•œ ë°ì´í„° ìŒ ë§¤ì¹­ ì¤‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
        
        for json_path in all_json_paths:
            try:
                # JSONì„ ì‚´ì§ ì—´ì–´ì„œ ë¹„ë””ì˜¤ íŒŒì¼ëª…ì„ í™•ì¸
                with open(json_path, 'r', encoding='utf-8') as f:
                    meta = json.load(f)
                
                # JSON ì•ˆì— ì íŒ ë¹„ë””ì˜¤ íŒŒì¼ëª…
                target_video_name = meta['metaData']['name']
                
                # ê·¸ íŒŒì¼ëª…ì´ ìš°ë¦¬ ë¹„ë””ì˜¤ ì§€ë„(Map)ì— ìˆë‹¤ë©´ í•©ê²©!
                if target_video_name in self.video_map:
                    self.json_paths.append(json_path)
                    
            except Exception:
                continue # JSONì´ ê¹¨ì¡Œê±°ë‚˜ í˜•ì‹ì´ ë‹¤ë¥´ë©´ íŒ¨ìŠ¤

        if len(self.json_paths) == 0:
            print(f"âŒ ê²½ê³ : ë§¤ì¹­ë˜ëŠ” ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        else:
            print(f"ğŸ‰ ìµœì¢… í•™ìŠµ ë°ì´í„°ì…‹ ì™„ì„±: ì´ {len(self.json_paths)}ê°œ ìŒ (ì˜ìƒ O, ë¼ë²¨ O)")

    def __len__(self):
        return len(self.json_paths)

    def __getitem__(self, idx):
        json_path = self.json_paths[idx]
        
        # 1. JSON íŒŒì¼ ë¡œë“œ
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # 2. ë¼ë²¨(í…ìŠ¤íŠ¸) ì¶”ì¶œ
        try:
            label_text = data['data'][0]['attributes'][0]['name']
        except (KeyError, IndexError):
            label_text = "Unknown"

        # 3. ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        video_filename = data['metaData']['name']
        
        # __init__ì—ì„œ ê²€ì¦í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„  ë¬´ì¡°ê±´ ì¡´ì¬í•œë‹¤ê³  ê°€ì • (ì•ˆì „)
        video_path = self.video_map[video_filename]

        # 4. ë¹„ë””ì˜¤ ë¡œë“œ
        frames = self._load_video(video_path)
        
        # 5. ì „ì²˜ë¦¬ ë° í…ì„œ ë³€í™˜
        if self.transform:
            # framesê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ë•Œë§Œ ë³€í™˜
            if len(frames) > 0:
                frames = torch.stack([self.transform(frame) for frame in frames])
            else:
                # ë§Œì•½ ì˜ìƒ ë¡œë“œì— ì‹¤íŒ¨í–ˆë‹¤ë©´ ë”ë¯¸ ë°ì´í„° ë°˜í™˜ (ì—ëŸ¬ ë°©ì§€)
                frames = torch.zeros((self.num_frames, 3, 224, 224))

        # (Frames, Channels, H, W) -> (Channels, Frames, H, W)
        # SLIP ëª¨ë¸ì€ (C, T, H, W)ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤.
        frames = frames.permute(1, 0, 2, 3) 

        return frames, label_text

    def _load_video(self, video_path):
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames <= 0:
            cap.release()
            # ì˜ìƒì´ ê¹¨ì¡Œì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê²€ì€ í™”ë©´ ë°˜í™˜
            return [Image.new('RGB', (224, 224)) for _ in range(self.num_frames)]

        if total_frames <= self.num_frames:
            frame_indices = list(range(total_frames))
        else:
            step = total_frames / self.num_frames
            frame_indices = [int(i * step) for i in range(self.num_frames)]
            
        frames = []
        current_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret: break
            if current_idx in frame_indices:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame))
                if len(frames) == self.num_frames: break
            current_idx += 1
        cap.release()
        
        # í”„ë ˆì„ ëª¨ìë¼ë©´ ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì‚¬í•´ì„œ ì±„ìš°ê¸°
        while len(frames) < self.num_frames:
            frames.append(frames[-1] if frames else Image.new('RGB', (224, 224)))
            
        return frames

# === í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì½”ë“œ ===
if __name__ == "__main__":
    from torchvision import transforms
    from torch.utils.data import DataLoader

    # ê²½ë¡œ ì„¤ì • (ì½”ë© í™˜ê²½)
    label_path = "/content/drive/MyDrive/Capstone/ìˆ˜ì–´ì˜ìƒ2/labels_01" 
    video_path = "/content/drive/MyDrive/Capstone/fin_videos_extracted"
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])

    try:
        dataset = SignLanguageDataset(label_dir=label_path, video_dir=video_path, transform=transform)
        
        # ì •ìƒì ìœ¼ë¡œ 5031ê°œê°€ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸
        print(f"ë°ì´í„°ì…‹ ê¸¸ì´: {len(dataset)}") 
        
        if len(dataset) > 0:
            frames, label = dataset[0]
            print(f"ì²« ë²ˆì§¸ ë°ì´í„°: {label}, {frames.shape}")
            
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬: {e}")