import torch
import os
import json
from tqdm import tqdm
from dataset import SignLanguageDataset
from encoder import SLIPVideoEncoder
from models import HybridTemporalModel
from torchvision import transforms

# ì„¤ì •
LABEL_DIR = "/content/drive/MyDrive/Capstone/morpheme/01"
VIDEO_DIR = "/content/drive/MyDrive/Capstone/fin_videos_extracted"
MODEL_PATH = "/content/drive/MyDrive/Capstone/slip_protonet_final.pth"
SAVE_PATH = "/content/drive/MyDrive/Capstone/prototypes.pt"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def create_prototypes():
    print(f"ğŸ› ï¸ í”„ë¡œí† íƒ€ì…(í´ë˜ìŠ¤ë³„ ê¸°ì¤€ì ) ìƒì„± ì‹œì‘... Device: {DEVICE}")

    # 1. ëª¨ë¸ ë¡œë“œ
    encoder = SLIPVideoEncoder(pretrained=False, embed_dim=512).to(DEVICE)
    temporal = HybridTemporalModel(input_dim=512, hidden_dim=512).to(DEVICE)
    
    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
    encoder.load_state_dict(checkpoint['encoder'])
    temporal.load_state_dict(checkpoint['temporal'])
    
    encoder.eval()
    temporal.eval()

    # 2. ë°ì´í„°ì…‹ ë¡œë“œ (ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ë´…ë‹ˆë‹¤)
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    dataset = SignLanguageDataset(LABEL_DIR, VIDEO_DIR, transform=transform)

    # 3. í´ë˜ìŠ¤ë³„ ì„ë² ë”© ëª¨ìœ¼ê¸°
    class_embeddings = {} # { "ì•ˆë…•í•˜ì„¸ìš”": [tensor1, tensor2...], "ê°ì‚¬í•©ë‹ˆë‹¤": [...] }
    
    MAX_SAMPLES_PER_CLASS=10
    unique_classes = set()
    print(f"ğŸ“Š ì „ì²´ ë°ì´í„° ì„ë² ë”© ì¶”ì¶œ ì¤‘... (í´ë˜ìŠ¤ë³„ ìµœëŒ€ {MAX_SAMPLES_PER_CLASS}ê°œ ì œí•œ ì ìš©)")
    with torch.no_grad():
        for i in tqdm(range(len(dataset))):
            try:
                img, label_path = dataset[i]
                
                # ë¼ë²¨ ì´ë¦„ ì¶”ì¶œ (JSON íŒŒì‹±)
                with open(label_path, 'r', encoding='utf-8') as f:
                    label_name = json.load(f)['data'][0]['attributes'][0]['name']
                unique_classes.add(label_name)
                
                if label_name in class_embeddings and len(class_embeddings[label_name]) >= MAX_SAMPLES_PER_CLASS:
                    continue
                img = img.unsqueeze(0).to(DEVICE) # (1, T, C, H, W)
                
                # íŠ¹ì§• ì¶”ì¶œ
                feats = encoder(img)
                emb = temporal(feats) # (1, 512)
                
                if label_name not in class_embeddings:
                    class_embeddings[label_name] = []
                class_embeddings[label_name].append(emb.cpu()) # CPUë¡œ ë‚´ë ¤ì„œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
            except Exception as e:
                continue
            all_classes_are_full = True
            for name in unique_classes:
                # ì•„ì§ ì´ í´ë˜ìŠ¤ê°€ class_embeddingsì— ì—†ê±°ë‚˜, ìƒ˜í”Œ ìˆ˜ê°€ 10ê°œ ë¯¸ë§Œì´ë©´
                if name not in class_embeddings or len(class_embeddings[name]) < MAX_SAMPLES_PER_CLASS:
                    all_classes_are_full = False
                    break
            # ëª¨ë“  í´ë˜ìŠ¤ê°€ 10ê°œ ì´ìƒ ëª¨ì˜€ë‹¤ë©´ ë£¨í”„ë¥¼ ì¢…ë£Œ
            if all_classes_are_full and len(unique_classes) > 0:
                 print("\nğŸ‰ ëª¨ë“  í´ë˜ìŠ¤ê°€ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ë¥¼ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤. í”„ë¡œí† íƒ€ì… ìƒì„± ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                 break

    # 4. í‰ê· (Prototype) ê³„ì‚°
    prototypes = {}
    print("ğŸ§® í´ë˜ìŠ¤ë³„ í‰ê·  ê³„ì‚° ì¤‘...")
    for label, emb_list in class_embeddings.items():
        # ìŠ¤íƒ í›„ í‰ê·  ê³„ì‚°
        emb_stack = torch.cat(emb_list, dim=0) # (N, 512)
        proto = emb_stack.mean(dim=0) # (512,) -> ì´ê²Œ ë°”ë¡œ ê·¸ í´ë˜ìŠ¤ì˜ ê¸°ì¤€ì !
        prototypes[label] = proto

    # 5. ì €ì¥
    torch.save(prototypes, SAVE_PATH)
    print(f"âœ… í”„ë¡œí† íƒ€ì… ì €ì¥ ì™„ë£Œ! ê²½ë¡œ: {SAVE_PATH}")
    print(f"ì´ {len(prototypes)}ê°œì˜ ìˆ˜ì–´ ë‹¨ì–´ ê¸°ì¤€ì ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    create_prototypes()