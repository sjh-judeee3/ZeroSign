import torch
import numpy as np
import os
import glob
import cv2
from tqdm import tqdm

# âœ… ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ ì„í¬íŠ¸ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
try:
    from src.encoder import SLIPVideoEncoder 
    from src.models import HybridTemporalModel, ProtoNetClassifier
except ImportError:
    from encoder import SLIPVideoEncoder
    from models import HybridTemporalModel, ProtoNetClassifier

# --- [ì„¤ì •] ---
DATA_ROOT = "eval_data_resized"                   # ì „ì²˜ë¦¬ëœ ë°ì´í„° í´ë”
CHECKPOINT_PATH = "checkpoint/slip_protonet_final.pth" # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ê²½ë¡œ
OUTPUT_PATH = "checkpoint/command_prototypes_demo.pt"       # ì €ì¥í•  í”„ë¡œí† íƒ€ì… íŒŒì¼ëª…
N_SUPPORT = 3                                     # í”„ë¡œí† íƒ€ì… ê³„ì‚°ì— ì‚¬ìš©í•  ì˜ìƒ ê°œìˆ˜
NUM_FRAMES = 16
EMBED_DIM = 512
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
if torch.backends.mps.is_available(): DEVICE = "mps"

# ==========================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (eval.pyì—ì„œ ì¬ì‚¬ìš©)
# ==========================================

# (load_video_tensor í•¨ìˆ˜ëŠ” ê¸¸ê¸° ë•Œë¬¸ì— ìƒëµí•˜ê³ , eval.pyì— ìˆëŠ” ê²ƒì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)
# (load_trained_models í•¨ìˆ˜ëŠ” ê¸¸ê¸° ë•Œë¬¸ì— ìƒëµí•˜ê³ , eval.pyì— ìˆëŠ” ê²ƒì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)
# [ì£¼ì˜] ì•„ë˜ ì½”ë“œì˜ í•¨ìˆ˜ë“¤ì€ eval.pyì—ì„œ ë³µì‚¬í•´ì™€ì•¼ í•©ë‹ˆë‹¤!

# ì˜ˆì‹œ: eval.pyì—ì„œ load_video_tensor, load_trained_models í•¨ìˆ˜ë¥¼ ë³µì‚¬í•´ì•¼ í•¨.

def load_video_tensor(video_path, num_frames=16):
    """
    MP4 íŒŒì¼ì„ ì½ì–´ [1, C, T, H, W] í…ì„œë¡œ ë³€í™˜
    """
    cap = cv2.VideoCapture(video_path)
    frames = []
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames <= 0:
        print(f"Error: Empty video {video_path}")
        return None

    # ê· ë“± ê°„ê²© ìƒ˜í”Œë§ (Uniform Sampling)
    if total_frames <= num_frames:
        indices = np.arange(total_frames)
    else:
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)
    
    current_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret: break
        
        if current_idx in indices:
            # BGR -> RGB & Normalize (0~1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = frame.astype(np.float32) / 255.0
            frames.append(frame)
            if len(frames) == num_frames:
                break
        current_idx += 1
    cap.release()

    # í”„ë ˆì„ ë¶€ì¡± ì‹œ ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì‚¬ (Padding)
    while len(frames) < num_frames:
        frames.append(frames[-1] if frames else np.zeros((224,224,3), dtype=np.float32))

    # Numpy -> Tensor ë³€í™˜
    # frames: [T, H, W, C] -> [C, T, H, W] (Model Input)
    frames = np.array(frames)
    frames = np.transpose(frames, (3, 0, 1, 2)) 
    
    return torch.tensor(frames).unsqueeze(0) # Batch ì°¨ì› ì¶”ê°€ [1, C, T, H, W]



def load_trained_models():
    print(f"ğŸ”„ Loading models on {DEVICE}...")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (train.pyì˜ íŒŒë¼ë¯¸í„°ì™€ ë™ì¼í•˜ê²Œ!)
    encoder = SLIPVideoEncoder(pretrained=False, embed_dim=EMBED_DIM).to(DEVICE)
    time_model = HybridTemporalModel(input_dim=EMBED_DIM, hidden_dim=EMBED_DIM).to(DEVICE)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if not os.path.exists(CHECKPOINT_PATH):
        raise FileNotFoundError(f"Checkpoints not found at {CHECKPOINT_PATH}")
        
    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)
    
    # train.pyì—ì„œ ì €ì¥í•œ í‚¤: 'encoder', 'temporal'
    print(f"âœ… Checkpoint Keys Found: {list(checkpoint.keys())}")
    
    try:
        encoder.load_state_dict(checkpoint['encoder'])
        time_model.load_state_dict(checkpoint['temporal'])
        print("âœ… Weights loaded successfully!")
    except KeyError as e:
        print(f"âŒ Key Error loading checkpoint: {e}")
        print("train.pyì˜ ì €ì¥ ì½”ë“œì™€ í‚¤ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None, None
    except Exception as e:
        print(f"âŒ Error loading weights: {e}")
        return None, None

    encoder.eval()
    time_model.eval()
    
    return encoder, time_model


# ==========================================
# 2. í”„ë¡œí† íƒ€ì… ê³„ì‚° ë° ì €ì¥
# ==========================================
def save_prototypes_for_demo():
    print(f"ğŸ”„ Loading models and calculating Prototypes (N={N_SUPPORT})...")
    
    encoder, time_model = load_trained_models()
    if encoder is None: return

    # 1. í´ë˜ìŠ¤ íƒìƒ‰
    all_classes = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])
    
    # Grab, Pinch, Pointë¥¼ í¬í•¨í•œ ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ ê³„ì‚° ì§„í–‰
    print(f"ğŸ“‚ Found Classes for Prototypes: {all_classes}")
    
    support_embs = []
    support_lbls = [] 
    class_map = [] # í´ë˜ìŠ¤ ì´ë¦„ê³¼ ì¸ë±ìŠ¤ ë§¤í•‘

    # 2. Support Set íŠ¹ì§• ì¶”ì¶œ
    for label_idx, class_name in enumerate(all_classes):
        class_dir = os.path.join(DATA_ROOT, class_name)
        video_files = sorted(glob.glob(os.path.join(class_dir, "*.mp4")))
        
        # ì•ì˜ N_SUPPORT ê°œ íŒŒì¼ë§Œ ì‚¬ìš©
        s_files = video_files[:N_SUPPORT]
        
        if len(s_files) < N_SUPPORT:
            print(f"âš ï¸  Warning: Class {class_name} has only {len(s_files)} videos. Skipping or padding.")
            continue
            
        print(f"   [Processing] {class_name} with {len(s_files)} videos...")
        
        # íŠ¹ì§• ì¶”ì¶œ
        for v_path in tqdm(s_files, desc=f"Feat. Extraction for {class_name}"):
            tensor = load_video_tensor(v_path, NUM_FRAMES)
            if tensor is None: continue
            tensor = tensor.to(DEVICE)
            
            with torch.no_grad():
                f_feat = encoder(tensor)
                vid_emb = time_model(f_feat)
                
            support_embs.append(vid_emb.cpu())
            support_lbls.append(label_idx)
        
        class_map.append(class_name)

    if not support_embs:
        print("âŒ ì¶”ì¶œëœ Support ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 3. Prototypes ê³„ì‚°
    S = torch.cat(support_embs)
    S_Y = torch.tensor(support_lbls)

    # ProtoNetClassifierì˜ compute_prototypes ë¡œì§ì„ ì¬ì‚¬ìš© (ìˆ˜ë™ ê³„ì‚°)
    dim = S.size(1)
    num_classes = len(class_map)
    prototypes = torch.zeros(num_classes, dim)
    
    for c in range(num_classes):
        class_samples = S[S_Y == c]
        if class_samples.size(0) > 0:
            prototypes[c] = class_samples.mean(dim=0)
            
    # 4. íŒŒì¼ ì €ì¥
    data_to_save = {
        'classes': class_map,
        'prototypes': prototypes.cpu(),
        'embedding_dim': EMBED_DIM,
        'N_support': N_SUPPORT
    }

    torch.save(data_to_save, OUTPUT_PATH)
    print(f"\nâœ… Success! Prototypes for {num_classes} classes saved.")
    print(f"ğŸ’¾ File Path: {os.path.abspath(OUTPUT_PATH)}")


if __name__ == "__main__":

    try:
        save_prototypes_for_demo()
    except NameError as e:
        print("\n--- ì‹¤í–‰ ì˜¤ë¥˜ ---")
        print(f"âŒ í•¨ìˆ˜ ì •ì˜ ì˜¤ë¥˜: {e}")
        print("eval.pyì˜ 'load_video_tensor' ë° 'load_trained_models' í•¨ìˆ˜ë¥¼ ë³µì‚¬í•˜ì—¬ ì´ íŒŒì¼ì— ì •ì˜í•œ í›„ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.")