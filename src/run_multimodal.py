import torch
import whisper
import os
from openai import OpenAI # í˜¹ì€ ë¡œì»¬ LLM ë¼ì´ë¸ŒëŸ¬ë¦¬
from encoder import SLIPVideoEncoder 
from models import HybridTemporalModel

# API í‚¤ ì„¤ì • (OpenAI ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
# os.environ["OPENAI_API_KEY"] = "sk-..." 

class MultimodalAgent:
    def __init__(self, model_path, proto_path, device="cuda"):
        self.device = device
        
        # 1. ìˆ˜ì–´ ëª¨ë¸ ë¡œë“œ
        self.encoder = SLIPVideoEncoder(pretrained=False, embed_dim=512).to(device)
        self.temporal = HybridTemporalModel(input_dim=512, hidden_dim=512).to(device)
        
        try:
            checkpoint = torch.load(model_path, map_location=device)
            self.encoder.load_state_dict(checkpoint['encoder'])
            self.temporal.load_state_dict(checkpoint['temporal'])
        except FileNotFoundError:
            print(f"âŒ ì˜¤ë¥˜: ëª¨ë¸ íŒŒì¼ '{model_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            raise
        self.encoder.eval()
        self.temporal.eval()

        # 2. í”„ë¡œí† íƒ€ì…(ê¸°ì¤€ì ) ë¡œë“œ - ì—¬ê¸°ê°€ í•µì‹¬ ë³€ê²½ì !
        print("ğŸ“‚ ìˆ˜ì–´ ê¸°ì¤€ì (Prototype) ë¡œë”© ì¤‘...")
        try:
            self.prototypes = torch.load(proto_path, map_location=device) 
        except FileNotFoundError:
            print(f"âŒ ì˜¤ë¥˜: í”„ë¡œí† íƒ€ì… íŒŒì¼ '{proto_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. make_prototypes.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            raise
        
        # ë”•ì…”ë„ˆë¦¬ë¥¼ í…ì„œ í–‰ë ¬ë¡œ ë³€í™˜ (ê³„ì‚° ì†ë„ë¥¼ ìœ„í•´)
        self.class_names = list(self.prototypes.keys())
        self.proto_matrix = torch.stack([self.prototypes[k] for k in self.class_names]).to(device) 
        # (Class_Num, 512)

        # 3. Whisper ë¡œë“œ
        print("ğŸ§ Whisper ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.whisper = whisper.load_model("base").to(device)

        # 4. LLM í´ë¼ì´ì–¸íŠ¸ (OpenAI ì˜ˆì‹œ, ë¡œì»¬ ëª¨ë¸ì´ë©´ transformers pipeline ì‚¬ìš©)
        self.client = OpenAI() 

    def predict_sign(self, video_tensor):
        """ì €ì¥ëœ í”„ë¡œí† íƒ€ì…ê³¼ ë¹„êµí•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ìˆ˜ì–´ ë‹¨ì–´ ì°¾ê¸°"""
        with torch.no_grad():
            video_tensor = video_tensor.to(self.device)
            features = self.encoder(video_tensor)
            query_emb = self.temporal(features) # (1, 512)

            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚° (Euclidean Distance)
            # (Query - Proto)^2
            dists = torch.cdist(query_emb, self.proto_matrix) # (1, Class_Num)
            
            # ê°€ì¥ ê±°ë¦¬ê°€ ì§§ì€ ì¸ë±ìŠ¤ ì°¾ê¸°
            min_dist_idx = torch.argmin(dists, dim=1).item()
            predicted_word = self.class_names[min_dist_idx]
            
            return predicted_word

    def generate_response(self, video_tensor, audio_path):
        # 1. ì¸ì‹ ìˆ˜í–‰
        sign_word = self.predict_sign(video_tensor)
        audio_result = self.whisper.transcribe(audio_path)['text']
        
        print(f"ğŸ‘€ ìˆ˜ì–´ ì¸ì‹: {sign_word}")
        print(f"ğŸ‘‚ ìŒì„± ì¸ì‹: {audio_result}")

        # 2. LLM í”„ë¡¬í”„íŠ¸ (Prompt Engineering)
        system_prompt = "ë‹¹ì‹ ì€ ì²­ê° ì¥ì• ì¸ê³¼ ë¹„ì¥ì• ì¸ì˜ ì†Œí†µì„ ë•ëŠ” í†µì—­ì‚¬ì…ë‹ˆë‹¤. ìˆ˜ì–´ ë‹¨ì–´ì™€ ìŒì„± í…ìŠ¤íŠ¸ê°€ ì£¼ì–´ì§€ë©´, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“œì„¸ìš”."
        
        user_prompt = f"""
        [ì…ë ¥ ì •ë³´]
        ìˆ˜ì–´ ë‹¨ì–´: {sign_word}
        ìŒì„± í…ìŠ¤íŠ¸: {audio_result}

        [ì§€ì‹œ ì‚¬í•­]
        1. ìˆ˜ì–´ ë‹¨ì–´ëŠ” í•µì‹¬ í‚¤ì›Œë“œì…ë‹ˆë‹¤.
        2. ìŒì„± í…ìŠ¤íŠ¸ê°€ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì§§ìœ¼ë©´ ìˆ˜ì–´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚´ìš©ì„ ë³´ì™„í•˜ì„¸ìš”.
        3. ë°˜ëŒ€ë¡œ ìˆ˜ì–´ ë‹¨ì–´ë§Œìœ¼ë¡œ ë¶€ì¡±í•˜ë©´ ìŒì„±ì„ ì°¸ê³ í•˜ì„¸ìš”.
        4. ê²°ê³¼ëŠ” 'í•´ì„ëœ ë¬¸ì¥' ë”± í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        5. ì‚¬ìš©ìê°€ ì–´ë–¤ ë°©í–¥ì„ ê°€ë¦¬í‚¤ê³  ìˆëŠ”ì§€ë„ ë§í•˜ì„¸ìš”.

        [ì˜ˆì‹œ 1]
        ìˆ˜ì–´: ë°°ê³ íŒŒ / ìŒì„±: ì—„ë§ˆ ë°¥
        í•´ì„: ì—„ë§ˆ, ì € ë°°ê³ íŒŒìš”. ë°¥ ì£¼ì„¸ìš”.

        [ì˜ˆì‹œ 2]
        ìˆ˜ì–´: ë³‘ì› / ìŒì„±: ë¨¸ë¦¬ê°€ ë„ˆë¬´ ì•„íŒŒ
        í•´ì„: ë¨¸ë¦¬ê°€ ë„ˆë¬´ ì•„íŒŒì„œ ë³‘ì›ì— ê°€ê³  ì‹¶ì–´ìš”.

        [ì‹¤ì œ ë¬¸ì œ]
        ìˆ˜ì–´: {sign_word} / ìŒì„±: {audio_result}
        í•´ì„:
        """

        # 3. LLM í˜¸ì¶œ
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo", # or gpt-4o
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        
        return response.choices[0].message.content

def main_run(model_path, proto_path, video_file, audio_file):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"ğŸš€ ë©€í‹°ëª¨ë‹¬ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘... (Device: {device})")
    try:
        agent = MultimodalAgent(model_path, proto_path, device)
    except FileNotFoundError as e:
        print(f"âŒ ëª¨ë¸ ë˜ëŠ” í”„ë¡œí† íƒ€ì… íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 1. ë¹„ë””ì˜¤ í…ì„œ ë¡œë“œ
    print(f"ğŸ“‚ ë¹„ë””ì˜¤ í…ì„œ ë¡œë”© ì¤‘: {video_file}")
    video_tensor = torch.load(video_file, map_location=device).float()
    
    # ëª¨ë¸ ì…ë ¥ í˜•íƒœ: (1, T, 3, H, W) ë˜ëŠ” (T, 3, H, W)
    # SLIP Encoderê°€ (T, 3, H, W)ë¥¼ ë°›ìœ¼ë©´, ë°°ì¹˜ ì°¨ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    if video_tensor.dim() == 4:
        # ProtoNetì€ ë³´í†µ ë°°ì¹˜ í¬ê¸° 1ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ [T, 3, H, W] -> [1, T, 3, H, W]
        video_tensor = video_tensor.unsqueeze(0) 

    # 2. ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ë° ì‘ë‹µ ìƒì„±
    print("ğŸ§  ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ë° LLM ì‘ë‹µ ìƒì„± ì‹œì‘...")
    try:
        llm_response = agent.generate_response(video_tensor, audio_file)
        
        print("\n" + "="*50)
        print("âœ¨ ìµœì¢… LLM í†µì—­ ê²°ê³¼:")
        print(llm_response.strip())
        print("="*50 + "\n")
        
    except Exception as e:
        print(f"âŒ ì¶”ë¡  ë˜ëŠ” LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì‹¤í–‰ ì˜ˆì‹œ (ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤)
if __name__ == '__main__':
    # âš ï¸ ì‚¬ìš©ìì˜ ìµœì¢… í•™ìŠµëœ ëª¨ë¸ ë° í”„ë¡œí† íƒ€ì… ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.
    MODEL_PATH = "slip_protonet_final.pth" 
    PROTO_PATH = "prototypes.pt"
    
    # motionCapture.pyê°€ ì €ì¥í•œ íŒŒì¼ ê²½ë¡œ
    VIDEO_FILE = "captured_video.pt" 
    AUDIO_FILE = "captured_audio.wav"
    
    # âš ï¸ ì£¼ì˜: ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— motionCapture.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ 
    # VIDEO_FILEê³¼ AUDIO_FILEì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    
    if os.path.exists(VIDEO_FILE) and os.path.exists(AUDIO_FILE):
        main_run(MODEL_PATH, PROTO_PATH, VIDEO_FILE, AUDIO_FILE)
    else:
        print(f"âš ï¸ {VIDEO_FILE} ë˜ëŠ” {AUDIO_FILE} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("motionCapture.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ìˆ˜ì–´ ë™ì‘ê³¼ ìŒì„±ì„ ìº¡ì²˜í•´ì£¼ì„¸ìš”.")

# ì‹¤í–‰ ì˜ˆì‹œ
# agent = MultimodalAgent("slip_protonet_final.pth", "prototypes.pt")
# print(agent.generate_response(dummy_video, "audio.mp3"))