import torch
import whisper
import os
from openai import OpenAI # í˜¹ì€ ë¡œì»¬ LLM ë¼ì´ë¸ŒëŸ¬ë¦¬
from encoder import SLIPVideoEncoder 
from models import HybridTemporalModel

# API í‚¤ ì„¤ì • (OpenAI ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
# os.environ["OPENAI_API_KEY"] = "sk-..." 

class MultimodalAgent:
    def __init__(self, model_path, proto_path, device="cuda"):
        self.device = device
        
        # 1. ìˆ˜ì–´ ëª¨ë¸ ë¡œë“œ
        self.encoder = SLIPVideoEncoder(pretrained=False, embed_dim=512).to(device)
        self.temporal = HybridTemporalModel(input_dim=512, hidden_dim=512).to(device)
        
        checkpoint = torch.load(model_path, map_location=device)
        self.encoder.load_state_dict(checkpoint['encoder'])
        self.temporal.load_state_dict(checkpoint['temporal'])
        self.encoder.eval()
        self.temporal.eval()

        # 2. í”„ë¡œí† íƒ€ì…(ê¸°ì¤€ì ) ë¡œë“œ - ì—¬ê¸°ê°€ í•µì‹¬ ë³€ê²½ì !
        print("ğŸ“‚ ìˆ˜ì–´ ê¸°ì¤€ì (Prototype) ë¡œë”© ì¤‘...")
        self.prototypes = torch.load(proto_path, map_location=device) 
        # self.prototypesëŠ” {"ì•ˆë…•í•˜ì„¸ìš”": tensor, "ë°°ê³ íŒŒ": tensor ...} í˜•íƒœ
        
        # ë”•ì…”ë„ˆë¦¬ë¥¼ í…ì„œ í–‰ë ¬ë¡œ ë³€í™˜ (ê³„ì‚° ì†ë„ë¥¼ ìœ„í•´)
        self.class_names = list(self.prototypes.keys())
        self.proto_matrix = torch.stack([self.prototypes[k] for k in self.class_names]).to(device) 
        # (Class_Num, 512)

        # 3. Whisper ë¡œë“œ
        self.whisper = whisper.load_model("base").to(device)

        # 4. LLM í´ë¼ì´ì–¸íŠ¸ (OpenAI ì˜ˆì‹œ, ë¡œì»¬ ëª¨ë¸ì´ë©´ transformers pipeline ì‚¬ìš©)
        self.client = OpenAI() 

    def predict_sign(self, video_tensor):
        """ì €ì¥ëœ í”„ë¡œí† íƒ€ì…ê³¼ ë¹„êµí•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ìˆ˜ì–´ ë‹¨ì–´ ì°¾ê¸°"""
        with torch.no_grad():
            video_tensor = video_tensor.to(self.device)
            features = self.encoder(video_tensor)
            query_emb = self.temporal(features) # (1, 512)

            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚° (Euclidean Distance)
            # (Query - Proto)^2
            dists = torch.cdist(query_emb, self.proto_matrix) # (1, Class_Num)
            
            # ê°€ì¥ ê±°ë¦¬ê°€ ì§§ì€ ì¸ë±ìŠ¤ ì°¾ê¸°
            min_dist_idx = torch.argmin(dists, dim=1).item()
            predicted_word = self.class_names[min_dist_idx]
            
            return predicted_word

    def generate_response(self, video_tensor, audio_path):
        # 1. ì¸ì‹ ìˆ˜í–‰
        sign_word = self.predict_sign(video_tensor)
        audio_result = self.whisper.transcribe(audio_path)['text']
        
        print(f"ğŸ‘€ ìˆ˜ì–´ ì¸ì‹: {sign_word}")
        print(f"ğŸ‘‚ ìŒì„± ì¸ì‹: {audio_result}")

        # 2. LLM í”„ë¡¬í”„íŠ¸ (Prompt Engineering)
        system_prompt = "ë‹¹ì‹ ì€ ì²­ê° ì¥ì• ì¸ê³¼ ë¹„ì¥ì• ì¸ì˜ ì†Œí†µì„ ë•ëŠ” í†µì—­ì‚¬ì…ë‹ˆë‹¤. ìˆ˜ì–´ ë‹¨ì–´ì™€ ìŒì„± í…ìŠ¤íŠ¸ê°€ ì£¼ì–´ì§€ë©´, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“œì„¸ìš”."
        
        user_prompt = f"""
        [ì…ë ¥ ì •ë³´]
        ìˆ˜ì–´ ë‹¨ì–´: {sign_word}
        ìŒì„± í…ìŠ¤íŠ¸: {audio_result}

        [ì§€ì‹œ ì‚¬í•­]
        1. ìˆ˜ì–´ ë‹¨ì–´ëŠ” í•µì‹¬ í‚¤ì›Œë“œì…ë‹ˆë‹¤.
        2. ìŒì„± í…ìŠ¤íŠ¸ê°€ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ì§§ìœ¼ë©´ ìˆ˜ì–´ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚´ìš©ì„ ë³´ì™„í•˜ì„¸ìš”.
        3. ë°˜ëŒ€ë¡œ ìˆ˜ì–´ ë‹¨ì–´ë§Œìœ¼ë¡œ ë¶€ì¡±í•˜ë©´ ìŒì„±ì„ ì°¸ê³ í•˜ì„¸ìš”.
        4. ê²°ê³¼ëŠ” 'í•´ì„ëœ ë¬¸ì¥' ë”± í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

        [ì˜ˆì‹œ 1]
        ìˆ˜ì–´: ë°°ê³ íŒŒ / ìŒì„±: ì—„ë§ˆ ë°¥
        í•´ì„: ì—„ë§ˆ, ì € ë°°ê³ íŒŒìš”. ë°¥ ì£¼ì„¸ìš”.

        [ì˜ˆì‹œ 2]
        ìˆ˜ì–´: ë³‘ì› / ìŒì„±: ë¨¸ë¦¬ê°€ ë„ˆë¬´ ì•„íŒŒ
        í•´ì„: ë¨¸ë¦¬ê°€ ë„ˆë¬´ ì•„íŒŒì„œ ë³‘ì›ì— ê°€ê³  ì‹¶ì–´ìš”.

        [ì‹¤ì œ ë¬¸ì œ]
        ìˆ˜ì–´: {sign_word} / ìŒì„±: {audio_result}
        í•´ì„:
        """

        # 3. LLM í˜¸ì¶œ
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo", # or gpt-4o
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7
        )
        
        return response.choices[0].message.content

# ì‹¤í–‰ ì˜ˆì‹œ
# agent = MultimodalAgent("slip_protonet_final.pth", "prototypes.pt")
# print(agent.generate_response(dummy_video, "audio.mp3"))